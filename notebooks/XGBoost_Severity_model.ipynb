{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models.video as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import re\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f75f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función que carga las etiquetas\n",
    "def load_action_labels(json_path):\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    action_labels = {}\n",
    "    for action_id, action_data in data['Actions'].items():\n",
    "        label = action_data['Severity']  # Extraer la etiqueta de 'Action class'\n",
    "        action_name = f\"action_{action_id}\"  # Formar el nombre de la acción\n",
    "        action_labels[action_name] = label\n",
    "    \n",
    "    return action_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3be6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('F:/MVFoulRecognition/features/Mvit/train/severity/end/train_combinados.pkl', 'rb') as file:  # 'rb' es lectura en modo binario\n",
    "    loaded_data = pickle.load(file)\n",
    "\n",
    "resultado_train = {k: v for dic in loaded_data for k, v in dic.items()}\n",
    "\n",
    "dic_ordenado_train = dict(sorted(resultado_train.items(), key=lambda x: int(x[0].split('_')[1])))\n",
    "\n",
    "#print(dic_ordenado_train)\n",
    "with open('F:/MVFoulRecognition/features/Mvit/test/test_combined.pkl', 'rb') as file:  # 'rb' es lectura en modo binario\n",
    "    loaded_data = pickle.load(file)\n",
    "\n",
    "resultado_test = {k: v for dic in loaded_data for k, v in dic.items()}\n",
    "\n",
    "# Salida: {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6}\n",
    "dic_ordenado_test = dict(sorted(resultado_test.items(), key=lambda x: int(x[0].split('_')[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b758d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = load_action_labels(\"F:/data/mvfouls/train/annotations.json\")\n",
    "ls = sorted(ls.items())\n",
    "sorted_labels_train = sorted(ls, key=lambda x: int(x[0].split('_')[1]))\n",
    "\n",
    "ls = load_action_labels(\"F:/data/mvfouls/test/annotations.json\")\n",
    "ls = sorted(ls.items())\n",
    "sorted_labels_test = sorted(ls, key=lambda x: int(x[0].split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de37c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobación de las acciones que no se han extraido características correctamente\n",
    "\n",
    "diccionario = {i: 0 for i in range(0, 4052)}\n",
    "\n",
    "for r in dic_ordenado_train:\n",
    "\n",
    "    action = int(r.split(\"_\")[1])  \n",
    "    if action in diccionario:\n",
    "        diccionario[action] = 1\n",
    "\n",
    "# Mostrar el diccionario actualizado\n",
    "diccionario_filtrado = {k: v for k, v in diccionario.items() if v == 0}\n",
    "print(diccionario_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos el número de acciones\n",
    "len(dic_ordenado_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a48ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AÑADIMOS NUEVAS ACCIONES AL DICCIONARIO DE LABELS \n",
    "\n",
    "\n",
    "\n",
    "# Calcular cuántas nuevas acciones tenemos hasta llegar a 3174 (Primer grupo de acciones aumentadas)\n",
    "new_actions_needed = 3174 - len(sorted_labels_train)+1\n",
    "\n",
    "# Generar nuevas acciones con la etiqueta '3.0'\n",
    "new_actions = [('action_' + str(i), '3.0') for i in range(len(sorted_labels_train), len(sorted_labels_train) + new_actions_needed)]\n",
    "\n",
    "# Agregar las nuevas acciones al diccionario\n",
    "updated_labels = sorted_labels_train + new_actions\n",
    "\n",
    "print(f\"Total de acciones después de la ampliación: {len(updated_labels)}\")\n",
    "print(updated_labels[:])  # Mostrar las últimas 10 acciones para verificar\n",
    "\n",
    "# Calcular cuántas nuevas acciones tenemos hasta llegar a 4052 (Segundo grupo de acciones aumentadas)\n",
    "new_actions_needed = 4052 - len(updated_labels)+1\n",
    "\n",
    "# Crear las nuevas acciones con etiqueta '5.0' desde el último índice actual\n",
    "start_index = int(updated_labels[-1][0].split('_')[1]) + 1  # último índice + 1\n",
    "\n",
    "# Generar nuevas acciones\n",
    "new_actions_5_0 = [('action_' + str(i), '5.0') for i in range(start_index, start_index + new_actions_needed)]\n",
    "\n",
    "# Agregar al listado actualizado\n",
    "updated_labels += new_actions_5_0\n",
    "\n",
    "# Verificación\n",
    "print(f\"Total final de acciones: {len(updated_labels)}\")\n",
    "print(\"Últimas acciones añadidas:\")\n",
    "print(updated_labels[-new_actions_needed:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e57fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos la distribución de etiquetas\n",
    "# Extraer solo las etiquetas\n",
    "only_labels = [label for (_, label) in sorted_labels_train]\n",
    "\n",
    "# Contar la frecuencia de cada clase\n",
    "print(Counter(only_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984726cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de mapeo\n",
    "conversion = {\n",
    "    \"1.0\": \"no card\",\n",
    "    \"2.0\": \"no card\",\n",
    "    \"3.0\": \"yellow card\",\n",
    "    \"4.0\": \"yellow card\",\n",
    "    \"5.0\": \"red card\",\n",
    "    \"\": \"\"  # dejamos las vacías tal cual\n",
    "}\n",
    "\n",
    "# Aplicar conversión\n",
    "updated_labels = [(action, conversion[label]) for action, label in updated_labels]\n",
    "\n",
    "# Verificamos algunas etiquetas convertidas\n",
    "print(sorted_labels_train[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc59768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos la distribución de etiquetas\n",
    "# Extraer solo las etiquetas\n",
    "only_labels = [label for (_, label) in sorted_labels_train]\n",
    "\n",
    "# Contar la frecuencia de cada clase\n",
    "print(Counter(only_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar las acciones con etiqueta '\"\"'\n",
    "offence_actions = [item for item in updated_labels if item[1] == \"\"]\n",
    "\n",
    "actions_to_remove = offence_actions\n",
    "\n",
    "actions_to_remove_list = actions_to_remove\n",
    "\n",
    "updated_labels = [item for item in updated_labels if item not in actions_to_remove_list]\n",
    "\n",
    "for action in actions_to_remove_list:\n",
    "    dic_ordenado_train.pop(action[0], None) \n",
    "print(\"Diccionario después de eliminar las acciones:\", dic_ordenado_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c827b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos la distribución de etiquetas\n",
    "# Extraer solo las etiquetas\n",
    "only_labels = [label for (_, label) in updated_labels]\n",
    "\n",
    "# Contar la frecuencia de cada clase\n",
    "print(Counter(only_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar las acciones con etiqueta 'no card'\n",
    "offence_actions = [item for item in updated_labels if item[1] == 'no card']\n",
    "\n",
    "# Seleccionar las ultima acciones con etiqueta 'no card'\n",
    "actions_to_remove = offence_actions[-500:]\n",
    "\n",
    "actions_to_remove_list = actions_to_remove\n",
    "\n",
    "updated_labels = [item for item in updated_labels if item not in actions_to_remove_list]\n",
    "\n",
    "for action in actions_to_remove_list:\n",
    "    dic_ordenado_train.pop(action[0], None) \n",
    "\n",
    "print(\"Diccionario después de eliminar las acciones:\", dic_ordenado_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb6a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos la distribución de etiquetas\n",
    "# Extraer solo las etiquetas\n",
    "only_labels = [label for (_, label) in updated_labels]\n",
    "\n",
    "# Contar la frecuencia de cada clase\n",
    "print(Counter(only_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las acciones que no se han extraido correctamente\n",
    "updated_labels = [item for item in updated_labels if item[0] != 'action_3320' and item[0] != 'action_3691' and item[0] != 'action_3848' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos la distribución de etiquetas de test\n",
    "# Extraer solo las etiquetas\n",
    "only_labels = [label for (_, label) in sorted_labels_test]\n",
    "\n",
    "# Contar la frecuencia de cada clase\n",
    "print(Counter(only_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0219d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos y agrupamos las etiquetas\n",
    "conversion = {\n",
    "    \"1.0\": \"no card\",\n",
    "    \"2.0\": \"no card\",\n",
    "    \"3.0\": \"yellow card\",\n",
    "    \"4.0\": \"yellow card\",\n",
    "    \"5.0\": \"red card\",\n",
    "    \"\": \"\"  # dejamos las vacías tal cual\n",
    "}\n",
    "\n",
    "# Aplicar conversión\n",
    "sorted_labels_test = [(action, conversion[label]) for action, label in sorted_labels_test]\n",
    "\n",
    "# Verificamos algunas etiquetas convertidas\n",
    "print(sorted_labels_test[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ead787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filtrar las acciones con etiqueta '\"\"'\n",
    "offence_actions = [item for item in sorted_labels_test if item[1] == \"\"]\n",
    "\n",
    "# 2. Seleccionar las primeras mil acciones con etiqueta '\"\"'\n",
    "# Aseguramos que hay al menos mil acciones con etiqueta '\"\"'\n",
    "actions_to_remove = offence_actions\n",
    "\n",
    "# 3. Guardar estas mil acciones en una lista\n",
    "actions_to_remove_list = actions_to_remove\n",
    "\n",
    "sorted_labels_test = [item for item in sorted_labels_test if item not in actions_to_remove_list]\n",
    "\n",
    "for action in actions_to_remove_list:\n",
    "    dic_ordenado_test.pop(action[0], None) \n",
    "# Ver el diccionario después de la eliminación\n",
    "print(\"Diccionario después de eliminar las acciones:\", dic_ordenado_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train  =np.array(list(dic_ordenado_train.values()))\n",
    "labels_dic_train = dict(updated_labels)\n",
    "Y_train = np.array(list(labels_dic_train.values()))\n",
    "\n",
    "X_test = np.array(list(dic_ordenado_test.values()))\n",
    "labels_dict_test = dict(sorted_labels_test)\n",
    "Y_test = np.array(list(labels_dict_test.values()))\n",
    "\n",
    "print(\"Ejemplo de etiquetas en Y_train:\", Y_train[:10])\n",
    "print(\"Ejemplo de etiquetas en Y_valid:\", Y_test[:10])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Ajustamos el codificador con TODAS las etiquetas posibles\n",
    "label_encoder.fit(np.concatenate([Y_train, Y_test]))\n",
    "\n",
    "# Transformamos las etiquetas de texto a números\n",
    "Y_train_encoded = label_encoder.transform(Y_train)\n",
    "Y_test_encoded = label_encoder.transform(Y_test)\n",
    "\n",
    "print(\"Etiquetas originales:\", np.unique(Y_train)) \n",
    "print(\"Etiquetas codificadas:\", np.unique(Y_train_encoded))\n",
    "print(\"Diccionario de conversión:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b5d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_valid shape: {X_test.shape}, Y_valid shape: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240cca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from scipy.stats import uniform\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Aplanar la segunda dimensión de X_train y X_valid\n",
    "X_train_flattened = X_train.squeeze(1)  # (2916, 400)\n",
    "X_test_flattened = X_test.squeeze(1)  # (301, 400)\n",
    "\n",
    "# Suponiendo que estos son tus datos completos\n",
    "X_total = np.concatenate((X_train_flattened, X_test_flattened))\n",
    "Y_total = np.concatenate((Y_train_encoded, Y_test_encoded))\n",
    "\n",
    "# Nueva división estratificada\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_total, Y_total,\n",
    "    test_size=0.2,  # o el porcentaje que quieras\n",
    "    stratify=Y_total,  # 👈 esto asegura la misma proporción de clases\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Aplicar SMOTE para balancear las clases en el conjunto de entrenamiento\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, Y_train_resampled = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],  # Número de árboles\n",
    "    'learning_rate': uniform(0.01, 0.2),         # Tasa de aprendizaje\n",
    "    'max_depth': [3, 4, 5, 6, 7],                # Profundidad máxima de los árboles\n",
    "    'min_child_weight': [1, 2, 3, 4],            # Peso mínimo de los hijos\n",
    "    'subsample': uniform(0.6, 0.4),              # Submuestra de las instancias\n",
    "    'colsample_bytree': uniform(0.6, 0.4),       # Submuestra de las columnas\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],                 # Penalización por complejidad\n",
    "    'class_weight': ['balanced', None],          # Manejo del desbalance\n",
    "}\n",
    "\n",
    "# Definir el modelo XGBoost\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Realizar Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,  # Número de combinaciones a probar\n",
    "    scoring='recall_weighted',  # Usar F1 macro como métrica de optimización\n",
    "    cv=3,  # Validación cruzada\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Para usar todos los núcleos del procesador\n",
    ")\n",
    "\n",
    "# Ajustar al conjunto de entrenamiento con las muestras balanceadas\n",
    "random_search.fit(X_train_resampled, Y_train_resampled)\n",
    "\n",
    "# Resultados de los mejores parámetros\n",
    "print(f\"Mejores parámetros encontrados: {random_search.best_params_}\")\n",
    "\n",
    "# Usar el mejor modelo\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "Y_test_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(f\"Precisión del modelo optimizado: {accuracy_score(Y_test, Y_test_pred)}\")\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(confusion_matrix(Y_test, Y_test_pred))\n",
    "print(\"Reporte de Clasificación:\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c61e26a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Guarda el modelo en un archivo\n",
    "with open('modelo_xgboost_severity.pkl', 'wb') as f:\n",
    "    pickle.dump(best_xgb, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb1fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad861c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
